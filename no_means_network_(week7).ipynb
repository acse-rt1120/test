{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "no_means_network_(week7).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPW68SeyIbsLe0ieSZHSODf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-rt1120/test/blob/main/no_means_network_(week7).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rheh7DUcXOH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIkvmboTcZ88"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from pycm import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import torch  # Pytorch\n",
        "import torch.nn as nn  # Neural network module\n",
        "import torch.nn.functional as fn  # Function module\n",
        "from torchvision import datasets  # Datasets from torchvision\n",
        "from torchvision import transforms  # Transforms from torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt  # Plotting using matplotlib\n",
        "import numpy as np  # Numpy\n",
        "from numpy import linalg\n",
        "device = 'cuda'  # Set out device to GPU\n",
        "\n",
        "print('done')  # Let me know this cell has finished"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxp0ImiMch6k"
      },
      "source": [
        "# MNIST Test dataset and dataloader declaration\n",
        "batch_size = 64\n",
        "data = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([transforms.ToTensor(),])\n",
        "                   ),\n",
        "                   batch_size=batch_size, shuffle=True) # Load MNIST. Use the Data Loader to shuffle and batch images\n",
        "\n",
        "images, labels = next(iter(data)) # A trick for getting a batch out of the dataloader object\n",
        "plt.imshow(images[0].squeeze()) # Show the first image from the batch\n",
        "plt.show()\n",
        "batch_num = len(data)\n",
        "data_size = len(data)*batch_size\n",
        "print('number of batches = '+str(len(data))) # Print num batches\n",
        "print('number of images = '+str(len(data)*batch_size)) # Print num images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O9xAZ5am8NI"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "x_data = np.array(\n",
        "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
        "x_data_torch = torch.from_numpy(x_data).float() \n",
        "\n",
        "y_data = np.array([0,1,2,0,0,2])\n",
        "y_data_torch = torch.from_numpy(y_data)\n",
        "\n",
        "num_features = 28*28\n",
        "num_classes = 10\n",
        "n_hidden_1 = 25\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHWh0C3MnGWs",
        "outputId": "10d8805f-9a84-4846-9446-c4a818f4f1b0"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "W1 = torch.randn(num_features, n_hidden_1, requires_grad=True)\n",
        "B1 = torch.randn(n_hidden_1, requires_grad=True)\n",
        "\n",
        "Wout = torch.randn(n_hidden_1, num_classes, requires_grad=True)\n",
        "Bout = torch.randn(num_classes, requires_grad=True)\n",
        "\n",
        "learning_rate=0.01\n",
        "no_of_epochs = 10\n",
        "\n",
        "for epoch in range(no_of_epochs):     #explicitly defne the forward and backward\n",
        "    z1 = torch.add(torch.matmul(x_data_torch, W1), B1) \n",
        "    Zout = torch.add(torch.matmul(nn.ReLU(z1), Wout), Bout)\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    # loss = criterion(a2, y) \n",
        "    log_softmax = F.log_softmax(Zout,dim=1)\n",
        "    loss = F.nll_loss(log_softmax, y_data_torch)\n",
        "\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        W1.data -= learning_rate*W1.grad.data\n",
        "        B1.data -= learning_rate*B1.grad.data\n",
        "        Wout.data -= learning_rate*Wout.grad.data\n",
        "        Bout.data -= learning_rate*Bout.grad.data\n",
        "\n",
        "    W1.grad.data.zero_()\n",
        "    B1.grad.data.zero_()\n",
        "    Wout.grad.data.zero_()\n",
        "    Bout.grad.data.zero_()\n",
        "    \n",
        "\n",
        "\n",
        "    if epoch % 200 == 199: \n",
        "        with torch.no_grad():\n",
        "            z1 = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "            Zout = torch.add(torch.matmul(nn.ReLU(z1) ,Wout),Bout)\n",
        "            predicted = torch.argmax(Zout, 1)\n",
        "            train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "            print('Epoch: %d, loss: %.4f, train_acc: %.3f' %(epoch + 1, loss.item() , train_acc))\n",
        "print(\"Finished\")\n",
        "# Result\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Truth :', y_data)\n",
        "print('Accuracy : %.2f' %train_acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 200, loss: 0.5847, train_acc: 0.833\n",
            "Epoch: 400, loss: 0.4327, train_acc: 0.833\n",
            "Epoch: 600, loss: 0.3225, train_acc: 0.833\n",
            "Epoch: 800, loss: 0.2372, train_acc: 1.000\n",
            "Epoch: 1000, loss: 0.1740, train_acc: 1.000\n",
            "Finished\n",
            "Predicted : [0 1 2 0 0 2]\n",
            "Truth : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyxRjwyHn_ka"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class ModelWithHiddenLayer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(ModelWithHiddenLayer, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        Zout = self.linear2(F.relu(z1))\n",
        "        return Zout\n",
        "\n",
        "model = ModelWithHiddenLayer(num_features, n_hidden_1, num_classes)\n",
        "\n",
        "learning_rate=0.01\n",
        "no_of_epochs = 1000\n",
        "\n",
        "# If you apply Pytorch’s CrossEntropyLoss to your output layer,\n",
        "# you get the same result as applying Pytorch’s NLLLoss to a LogSoftmax layer added after your original output layer.\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UPLKBWKoE8r",
        "outputId": "2ce71b89-4229-4a33-874f-a5dfcbcc694f"
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "layers_num = -1\n",
        "layer_size = []\n",
        "for param_tensor in model.state_dict():\n",
        "    layers_num += 1\n",
        "    layer_size.append(list(model.state_dict()[param_tensor].size()))\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "linear1.weight \t torch.Size([5, 2])\n",
            "linear1.bias \t torch.Size([5])\n",
            "linear2.weight \t torch.Size([3, 5])\n",
            "linear2.bias \t torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "EjlvTLWKof0p",
        "outputId": "f27f8954-a8fd-4894-dcab-8529d682bd3c"
      },
      "source": [
        "for epoch in range(no_of_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    # get the inputs\n",
        "    inputs = x_data_torch\n",
        "    labels = y_data_torch\n",
        "\n",
        "    model.train()\n",
        "    # zero the parameter gradients\n",
        "    for param in model.parameters():\n",
        "      param.grad.zero_()\n",
        "    # linear1.weight.grad.data.zero_()\n",
        "    # linear1.bias.grad.data.zero_()\n",
        "    # linear2.weight.grad.data.zero_()\n",
        "    # linear2.bias.grad.data.zero_()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels) # We don't need to calcualte logsoftmax here\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        linear1.weight.data -= learning_rate*linear1.weight.grad.data\n",
        "        linear1.bias.data -= learning_rate*linear1.bias.grad.data\n",
        "        linear2.weight.data -= learning_rate*linear2.weight.grad.data\n",
        "        linear2.bias.data -= learning_rate*linear2.bias.grad.data\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 200 == 199:    # print every 200 epochs\n",
        "        model.eval()\n",
        "        pred_outputs = model(inputs)\n",
        "        predicted = torch.argmax(pred_outputs, 1)\n",
        "        train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "        print('%d, loss: %.4f, train_acc: %.4f' %(epoch + 1, loss.item(), train_acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Result\n",
        "pred_outputs = model(inputs)\n",
        "_, predicted = torch.max(pred_outputs, 1)\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Truth :', y_data)\n",
        "\n",
        "train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "print('Accuracy : %.2f' %train_acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-59e157f1562e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# linear1.weight.grad.data.zero_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# linear1.bias.grad.data.zero_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_61WT9mjoU8b"
      },
      "source": [
        "for epoch in range(no_of_epochs):     #explicitly defne the forward and backward\n",
        "    z1 = torch.add(torch.matmul(x_data_torch, W1), B1) \n",
        "    Zout = torch.add(torch.matmul(F.relu(z1), Wout), Bout) \n",
        "\n",
        "    log_softmax = F.log_softmax(Zout,dim=1)\n",
        "    loss = F.nll_loss(log_softmax, y_data_torch)\n",
        "\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        W1.data -= learning_rate*W1.grad.data\n",
        "        B1.data -= learning_rate*B1.grad.data\n",
        "        Wout.data -= learning_rate*Wout.grad.data\n",
        "        Bout.data -= learning_rate*Bout.grad.data\n",
        "\n",
        "    W1.grad.data.zero_()\n",
        "    B1.grad.data.zero_()\n",
        "    Wout.grad.data.zero_()\n",
        "    Bout.grad.data.zero_()\n",
        "    \n",
        "\n",
        "\n",
        "    if epoch % 200 == 199: \n",
        "        with torch.no_grad():\n",
        "            z1 = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "            Zout = torch.add(torch.matmul(F.relu(z1) ,Wout),Bout)\n",
        "            predicted = torch.argmax(Zout, 1)\n",
        "            train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "            print('Epoch: %d, loss: %.4f, train_acc: %.3f' %(epoch + 1, loss.item() , train_acc))\n",
        "print(\"Finished\")\n",
        "# Result\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Truth :', y_data)\n",
        "print('Accuracy : %.2f' %train_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}